{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea72b16-3ae8-4760-85fe-4ac01df2fb96",
   "metadata": {},
   "source": [
    "**PAGED ATTENTION**: An attention algorithm that allows for storing continuous keys and values in non-contiguous memory space.\n",
    "\n",
    "\n",
    "\n",
    "*   KV cache ofeach sequence is partitioned into blocks\n",
    "*   Each block has block_size = number of tokens\n",
    "*   During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22b8f1-7d1d-406a-b332-1895a536e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699d57c-5aa9-486a-ae37-b44f85d61d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vllm import LLMEngine, SamplingParams, TextCompletionParams\n",
    "\n",
    "# Initialize the LLM engine with PagedAttention\n",
    "engine = LLMEngine(model=\"gpt2-large\", use_paged_attention=True)\n",
    "\n",
    "# Define the sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "# Define the text completion parameters\n",
    "completion_params = TextCompletionParams(\n",
    "    prompt=\"Once upon a time\",\n",
    "    num_completions=3\n",
    ")\n",
    "\n",
    "# Generate text completions\n",
    "completions = engine.generate(completion_params, sampling_params)\n",
    "\n",
    "# Print the generated completions\n",
    "for i, completion in enumerate(completions):\n",
    "    print(f\"Completion {i+1}: {completion['text']}\")\n",
    "\n",
    "# Clean up\n",
    "engine.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49defd97-c4e5-48e2-b739-4dfa7fb10abf",
   "metadata": {},
   "source": [
    "# Advantages of vLLM\n",
    "\n",
    "vLLM offers several advantages, particularly in serving large language models (LLMs). Here are some key benefits:\n",
    "\n",
    "## 1. Memory Efficiency\n",
    "- **PagedAttention**: One of the standout features of vLLM is PagedAttention, which partitions the key-value (KV) cache into blocks, allowing for non-contiguous memory storage. This technique reduces memory fragmentation and optimizes memory usage, enabling the system to handle larger batches of requests and improve GPU utilization.\n",
    "\n",
    "## 2. High Throughput\n",
    "- vLLM achieves significantly higher throughput than traditional frameworks like Hugging Face Transformers and TensorFlow-based inference systems. This is achieved by efficiently managing memory and computation, allowing for faster processing of multiple requests simultaneously. Studies have shown that vLLM can achieve up to 15x higher throughput than Hugging Face Transformers and 3.5x higher than other optimized systems.\n",
    "\n",
    "## 3. Scalability\n",
    "- vLLM is designed to scale efficiently, supporting a wide range of models from small to very large. This scalability is crucial for serving popular and resource-intensive models like GPT-3, LLaMA, and other state-of-the-art LLMs. The efficient memory management and high throughput ensure that the system can handle increased traffic without significant degradation in performance.\n",
    "\n",
    "## 4. Flexible Memory Management\n",
    "- The PagedAttention mechanism allows for flexible memory management by efficiently sharing memory resources among different sequences. This flexibility is particularly beneficial in parallel sampling and beam search, where memory usage can be significantly reduced by up to 55%, thus improving throughput and making complex sampling methods more practical.\n",
    "\n",
    "## 5. Reduced Latency\n",
    "- By optimizing memory usage and reducing the overhead associated with memory management, vLLM can lower the latency of generating responses. This is crucial for applications requiring real-time or near-real-time interactions, such as chatbots and interactive AI systems.\n",
    "\n",
    "## 6. Ease of Use\n",
    "- vLLM provides an easy-to-use interface and integrates well with existing frameworks. This makes it accessible for developers and researchers who can leverage its capabilities without significant changes to their existing workflows. The documentation and support for various deployment options further enhance its usability.\n",
    "\n",
    "## 7. Cost Efficiency\n",
    "- By improving memory and compute efficiency, vLLM can reduce the operational costs of running large language models. This is particularly important for organizations looking to deploy large-scale LLMs cost-effectively.\n",
    "\n",
    "In summary, vLLM offers significant advantages in terms of memory efficiency, throughput, scalability, flexible memory management, reduced latency, ease of use, and cost efficiency, making it a powerful tool for serving large language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bd210-6fef-4700-9f18-32fb1ed43f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
